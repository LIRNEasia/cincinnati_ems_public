{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Updates\n",
    "Update dictionary based on code severities, their weighting, and observed fraction of transports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import utils.pg_tools as pg\n",
    "import luigi, luigi.configuration\n",
    "import numpy as np\n",
    "\n",
    "pgw = pg.PGWrangler(dbitems = pg.get_pgdict_from_cfg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "luigi_config = luigi.configuration.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buckets = {}\n",
    "for x in luigi_config.get('RunModels','code_buckets').split('-')[1:]:\n",
    "    (key, value) = x.strip().split(':')\n",
    "    buckets[key.strip()] = [y.strip() for y in value.strip().split(',') if not y is '']\n",
    "bucket_bycode = {}\n",
    "for key in buckets:\n",
    "    for val in buckets[key]:\n",
    "        bucket_bycode[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucket_weights = {}\n",
    "for x in luigi_config.get('DictOptimization', 'code_weights').split('- ')[1:]:\n",
    "    (key, value) = x.strip().split(':')\n",
    "    bucket_weights[key.strip()] = np.array([y.strip() for y in value.strip().split(' ')]).reshape((2,2)).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights for each severity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read as:\n",
    "\n",
    "```[[ tp, fn]\n",
    " [ fp, tp]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Weighting per class\")\n",
    "for key in bucket_weights:\n",
    "    if not key=='other':\n",
    "        print(key)\n",
    "        print(bucket_weights[key])\n",
    "        print(buckets[key])\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = pgw.get_conn()\n",
    "train_years = (2013, 2014)\n",
    "test_years = (2015, 2015)\n",
    "sql = \"\"\"\n",
    "      select dispatch_code, code_type, code_level, code_rest, \n",
    "          avg(trns_to_hosp::int) as frac_trans, \n",
    "          count(incident) as n_incidents,\n",
    "          avg(m_required::int) as mt_required \n",
    "      from semantic.master\n",
    "      where time_year >= %d and time_year <= %d\n",
    "      group by dispatch_code, code_type, code_level, code_rest\n",
    "      order by count(incident) DESC\n",
    "      \"\"\"\n",
    "train_df = pd.read_sql(sql%train_years, conn)\n",
    "test_df = pd.read_sql(sql%test_years, conn)\n",
    "conn.close()\n",
    "assert(set(train_df.mt_required.unique()) == {0,1} and set(test_df.mt_required.unique()) == {0,1})\n",
    "train_df.mt_required = train_df.mt_required.astype('bool')\n",
    "test_df.mt_required = test_df.mt_required.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.set_index('dispatch_code', inplace=True)\n",
    "test_df.set_index('dispatch_code', inplace=True)\n",
    "common_codes = set(train_df.index) & set(test_df.index)\n",
    "train_df = train_df.ix[list(common_codes)]\n",
    "train_df.sort_values(by='n_incidents', ascending=False, inplace=True)\n",
    "test_df = test_df.ix[list(common_codes)]\n",
    "test_df.sort_values(by='n_incidents', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute suggested transport\n",
    "Suggest whenever the score according to the severity weighting is higher for transport vs. no transport for all cases of this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bucket_f(code):\n",
    "    try:\n",
    "        return bucket_bycode[code]\n",
    "    except:\n",
    "        return 'other'\n",
    "train_df.insert(5,'urgency', train_df['code_type'].apply(bucket_f))\n",
    "test_df.insert(5,'urgency', test_df['code_type'].apply(bucket_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def suggest_transport(row):\n",
    "    if row.urgency == 'high':\n",
    "        return True\n",
    "    \n",
    "    tp_sent = row.frac_trans\n",
    "    fp_sent = 1-tp_sent\n",
    "    tn_sent = 0\n",
    "    fn_sent = 0\n",
    "    confusion_sent = np.array([[tp_sent, fn_sent],[fp_sent, tn_sent]])\n",
    "\n",
    "    tp_nots = 0\n",
    "    fp_nots = 0\n",
    "    tn_nots = 1-row.frac_trans\n",
    "    fn_nots = 1-tn_nots\n",
    "    confusion_nots = np.array([[tp_nots, fn_nots],[fp_nots, tn_nots]])\n",
    "\n",
    "    urgency = row.urgency\n",
    "    \n",
    "    score_sent = (bucket_weights[urgency]*confusion_sent).sum()\n",
    "    score_nots = (bucket_weights[urgency]*confusion_nots).sum()\n",
    "    \n",
    "    return score_sent>score_nots\n",
    "    \n",
    "train_df['mt_suggested'] = train_df.apply(suggest_transport, axis=1)\n",
    "test_df['mt_suggested'] = train_df['mt_suggested']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Most frequent codes:\")\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = 'mt_required'\n",
    "k_df = train_df[['n_incidents', key, 'urgency']]\n",
    "k_df['total_required'] = k_df[key]*k_df['n_incidents']\n",
    "urgency_grouped = k_df.groupby('urgency')\n",
    "print(\"Fraction transported for each urgency level according to current dispatch protocol:\")\n",
    "print(urgency_grouped['total_required'].sum()/urgency_grouped['n_incidents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting increase in transport runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_mt_dispatches_suggested = sum(test_df.mt_suggested*test_df.n_incidents)\n",
    "N_mt_dispatches = int(sum(test_df.mt_required*test_df.n_incidents))\n",
    "mt_dispatches_increase = (N_mt_dispatches_suggested-N_mt_dispatches)/float(N_mt_dispatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_transports_suggested = N_mt_dispatches_suggested + sum(~test_df.mt_suggested*test_df.frac_trans*test_df.n_incidents)\n",
    "N_transports = N_mt_dispatches + sum(~test_df.mt_required*test_df.frac_trans*test_df.n_incidents)\n",
    "mt_transport_increase = (N_transports_suggested-N_transports)/float(N_transports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Suggested inrease in mt dispatches: %.2f\"%(mt_dispatches_increase))\n",
    "print(\"Resulting increase in transport runs: %.2f\"%(mt_transport_increase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(col):\n",
    "    tp = sum(test_df.n_incidents*test_df[col]*test_df.frac_trans)\n",
    "    fn = sum(test_df.n_incidents*(~test_df[col])*test_df.frac_trans)\n",
    "    fp = sum(test_df.n_incidents*test_df[col]*(1-test_df.frac_trans))\n",
    "    tn = sum(test_df.n_incidents*(~test_df[col])*(1-test_df.frac_trans))\n",
    "    matrix = np.array([[tp, fn],[fp, tn]])\n",
    "    matrix /= matrix.sum()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_old = confusion_matrix('mt_required')\n",
    "print(\"Old confusion matrix:\")\n",
    "print(conf_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix based on suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_new = confusion_matrix('mt_suggested')\n",
    "print(\"New confusion matrix:\")\n",
    "print(conf_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incidents_year = sum(test_df.n_incidents) #2015\n",
    "print(\"Absolute effect for the test set (2015)\")\n",
    "print((conf_new-conf_old)*incidents_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All proposed changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = \"dispatch_dictionary.xls\"\n",
    "ew = pd.ExcelWriter(fname)\n",
    "test_df.to_excel(ew, \"Full data\")\n",
    "test_df[['mt_suggested']].to_excel(ew, \"New dictionary\")\n",
    "test_df[~(test_df['mt_suggested']==test_df['mt_required'])].to_excel(ew, \"All changes\")\n",
    "print(\"Saved results to \"+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df[~(test_df['mt_suggested']==test_df['mt_required'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
