{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import operator\n",
    "import psycopg2\n",
    "import pylab\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats.stats import pearsonr\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn import metrics\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from statsmodels.tsa import stattools\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "import random\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.mlab as mlab\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import statsmodels\n",
    "from statsmodels.graphics.api import qqplot\n",
    "\n",
    "mpl.rcdefaults()\n",
    "pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read database parameters from default_profile\n",
    "dbitems = {}\n",
    "with open('default_profile') as f:\n",
    "    for line in f.readlines():\n",
    "        item = line.split(\" \")[1].split(\"=\")\n",
    "        dbitems[item[0]] = item[1].strip()\n",
    "        \n",
    "# Connect to database with psycopg2\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='%s' user='%s' host='%s' password='%s'\"%(dbitems['PGDATABASE'],dbitems['PGUSER'],dbitems['PGHOST'],dbitems['PGPASSWORD']))\n",
    "except:\n",
    "    print \"Unable to connect to the database\"\n",
    "    \n",
    "# Connect to database with sqalchemy\n",
    "conn_sqlalch = create_engine('postgresql+psycopg2://%s:%s@%s/%s'%(dbitems['PGUSER'],dbitems['PGPASSWORD'],dbitems['PGHOST'],dbitems['PGDATABASE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_df = pd.read_sql_query(\"SELECT * FROM semantic_demand_R.master\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvar = 'trns_to_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_df = time_df.groupby(['time_year', 'time_month', 'time_day', 'station_name']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert hourly data to daily data\n",
    "day_df.reset_index(inplace = True)\n",
    "\n",
    "day_df['time'] = day_df.apply(lambda x: datetime.datetime(x.time_year, x.time_month, x.time_day), axis = 1)\n",
    "\n",
    "day_df.set_index('time', inplace = True)\n",
    "\n",
    "day_df.drop(['time_year', 'time_month', 'time_day', 'time_hour'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def metrics_reg(y_true, y_pred, final_weight = 0.1, schedule = 'exp', overunder = (1,1)):\n",
    "    \"\"\"\n",
    "    input: true y vals, predicted y vals, final weight, decay schedule, weights for over/undersending\n",
    "    output: (Mean Absolute Percent Error, Mean Squared Error, Mean Absolute Error, Time Weighted Score)\n",
    "    \"\"\"\n",
    "    #calculate mean absolute percent error\n",
    "    mape = np.mean([abs(item[1] - item[0])/float(item[0]) for item in zip(y_true, y_pred)])\n",
    "    #calculate mean squared error\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)**0.5\n",
    "    #calculate mean absolute error\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    #create time decaying weights based on decay schedule\n",
    "    if schedule == 'exp':\n",
    "        k = np.log(1/float(final_weight))/(len(y_true)-1)\n",
    "        tweights = [np.exp(-k*i) for i in range(len(y_true))]\n",
    "    elif schedule == 'lin':\n",
    "        tweights = [(i+1)*(final_weight-1)/float(len(y_true)) + 1 for i in range(len(y_true))]\n",
    "        \n",
    "    \n",
    "    #generate time weighted score taking into account penalties for over and under sending\n",
    "    \n",
    "    twscore = np.mean([((overunder[0]-overunder[1])*(int(item[0] < item[1])) + overunder[1])\n",
    "                       *(abs(item[1] - item[0])/float(item[0]))\n",
    "                       *(tweights[item[2]])\n",
    "                       for item in zip(y_test, y_pred, range(len(y_pred))) if item[0] != 0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (mape, mse, mae, twscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, Lars, ElasticNet, SGDRegressor, LassoLars, Perceptron, BayesianRidge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#models to use\n",
    "models = [Lasso(), Ridge(), LinearRegression(), Lars(), ElasticNet(), LassoLars(), Perceptron(), BayesianRidge()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#which results to expect\n",
    "df_results = pd.DataFrame(columns = ['model', 'station', 'MAPE', 'MSE', 'MAE', 'TWSCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[train_df.trns_to_hosp == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for each station in the city, run all models and gather results\n",
    "i = 0\n",
    "for station in set(day_df.station_name):\n",
    "    for model in models:\n",
    "    \n",
    "        day_df_sta = day_df[day_df.station_name == station].drop('station_name', axis =1)\n",
    "\n",
    "        day_df_sta['LagInc'] = day_df_sta.total_incidents.shift()\n",
    "        \n",
    "        day_df_sta = day_df_sta[day_df_sta[yvar] != 0]\n",
    "\n",
    "        day_df_sta.dropna(inplace = True)\n",
    "\n",
    "        train_df = day_df_sta[(day_df_sta.index.year <= 2014) & (day_df_sta.index.year >= 2013)]\n",
    "        test_df = day_df_sta[day_df_sta.index.year ==2015]\n",
    "\n",
    "        X_train = train_df[['LagInc']].as_matrix()\n",
    "        X_test = test_df[['LagInc']].as_matrix()\n",
    "        y_train = train_df[yvar]\n",
    "        y_test = test_df[yvar]\n",
    "\n",
    "        clf = model\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        mod_metrics = metrics_reg(y_test, y_pred, final_weight = 0.1, schedule = 'exp', overunder = (1,5))\n",
    "\n",
    "        df_results.loc[i] = [model, station, mod_metrics[0], mod_metrics[1], mod_metrics[2], mod_metrics[3]]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,group in df_results.groupby(['station']):\n",
    "    print name\n",
    "    print group.sort('TWSCORE').iloc[0][['model', 'TWSCORE']]\n",
    "    print '-----'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_df = pd.read_sql_query(\"SELECT * FROM features_demand.master\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#determine which aggegation function to use for each column\n",
    "dict_func = {}\n",
    "for col in time_df:\n",
    "    if col in ['m_required', 'trns_to_hosp']:\n",
    "        dict_func[col] = np.sum\n",
    "    elif col not in ['station_name', '_date', 'pk_demand', 'time_of_day']:\n",
    "        dict_func[col] = np.mean\n",
    "\n",
    "day_df = time_df.groupby(['_date', 'station_name']).agg(dict_func)\n",
    "\n",
    "day_df.reset_index(inplace = True)\n",
    "\n",
    "day_df['time'] = day_df.apply(lambda x: datetime.datetime(int(x.time_year), int(x.time_month), int(x.time_day)), axis = 1)\n",
    "\n",
    "\n",
    "day_df.drop(['time_year', 'time_month', 'time_day'], axis = 1, inplace = True)\n",
    "\n",
    "day_df.drop('_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_file = open('/mnt/data/cincinnati/model_pickle/c0666e89e1df717913a6d802f55af15a.p', 'rb')\n",
    "\n",
    "data1 = pickle.load(pkl_file)\n",
    "\n",
    "data1.set_index('incident', inplace = True)\n",
    "\n",
    "feat_df = pd.read_sql_query(\"SELECT incident, trns_to_hosp, time_year, m_required FROM features.master_tmp\", conn)\n",
    "\n",
    "feat_df = feat_df[feat_df.time_year == 2015]\n",
    "\n",
    "feat_df.set_index('incident', inplace = True)\n",
    "\n",
    "feat_df['score'] = data1.score\n",
    "\n",
    "feat_df.dropna(inplace = True)\n",
    "\n",
    "tp_curr = len(feat_df[(feat_df.trns_to_hosp == True)&(feat_df.m_required == True)])\n",
    "fp_curr = len(feat_df[(feat_df.trns_to_hosp == False)&(feat_df.m_required == True)])\n",
    "fn_curr = len(feat_df[(feat_df.trns_to_hosp == True)&(feat_df.m_required == False)])\n",
    "tn_curr = len(feat_df[(feat_df.trns_to_hosp == False)&(feat_df.m_required == False)])\n",
    "\n",
    "pct_const = 100*(1 - float(tn_curr)/(tp_curr + tn_curr + fn_curr + fp_curr)) - 7.2\n",
    "mod_lim = np.percentile(feat_df.score, 100 - pct_const)\n",
    "\n",
    "tp_mod = len(feat_df[(feat_df.trns_to_hosp == True)&(feat_df.score >= mod_lim)])\n",
    "fp_mod = len(feat_df[(feat_df.trns_to_hosp == False)&(feat_df.score >= mod_lim)])\n",
    "fn_mod = len(feat_df[(feat_df.trns_to_hosp == True)&(feat_df.score < mod_lim)])\n",
    "tn_mod = len(feat_df[(feat_df.trns_to_hosp == False)&(feat_df.score < mod_lim)])\n",
    "\n",
    "sum_inc = float(sum([tp_curr, fp_curr, fn_curr, tn_curr]))\n",
    "\n",
    "print [tp_curr/sum_inc, fp_curr/sum_inc, fn_curr/sum_inc, tn_curr/sum_inc]\n",
    "\n",
    "print [tp_mod/sum_inc, fp_mod/sum_inc, fn_mod/sum_inc, tn_mod/sum_inc]\n",
    "\n",
    "tn_mod - tn_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT * FROM features_demand.master\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hour_lag_demand = []\n",
    "for i,row in df.iterrows():\n",
    "    try:\n",
    "        hour_lag_demand.append(df[(df.time_day == row.time_day - 1)&(df.station_name == row.station_name)])['trns_tp_hosp'].iloc[0]\n",
    "    except TypeError:\n",
    "        hour_lag_demand.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.pk_demand.apply(lambda x: datetime.datetime(x.replace('-',' ').replace(':', ' ').split('_')[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.replace('-',' ').replace(':', ' ').split('_')[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_df = pd.read_sql_query(\"SELECT * FROM features_demand.master\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_sql_query(\"SELECT * FROM model_demand.training\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create multiindex of station and times to fater access data\n",
    "ind_zip = zip(train_df.time, train_df.station_name)\n",
    "\n",
    "index_ts = pd.MultiIndex.from_tuples(ind_zip, names=['day', 'station'])\n",
    "\n",
    "train_df.index = index_ts\n",
    "\n",
    "l = []\n",
    "for i,row in train_df.iterrows():\n",
    "    try:\n",
    "        l.append(train_df.ix[i[0]-datetime.timedelta(1)].ix[i[1]].trns_to_hosp)\n",
    "    except KeyError:\n",
    "        l.append(0)\n",
    "train_df['lag1'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    input: real y's and y probabilities\n",
    "    output: a plot of precision and recall at k\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1)\n",
    "\n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train Set Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bring in model scores \n",
    "model_scores = pd.read_csv('/mnt/data/cincinnati/mkiang/cincinnati_ems/yes_time_training_model_scores.csv')\n",
    "model_scores.index.rename('incident', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#populate dictionary with each model and which bucket it falls into\n",
    "dict_buckets = {'low':[], 'med':[], 'high':[]}\n",
    "import random\n",
    "l = model_scores.columns.tolist()\n",
    "random.shuffle(l)\n",
    "for col in l:\n",
    "    if 'm_' in col:\n",
    "        low_slice = model_scores[model_scores.bucket == 'low'][col].dropna()\n",
    "        med_slice = model_scores[model_scores.bucket == 'medium'][col].dropna()\n",
    "        high_slice = model_scores[model_scores.bucket == 'high'][col].dropna()\n",
    "        \n",
    "        if len(low_slice) > 0:\n",
    "            dict_buckets['low'].append(col)\n",
    "        elif len(med_slice) > 0:\n",
    "            dict_buckets['med'].append(col)\n",
    "        elif len(high_slice) > 0:\n",
    "            dict_buckets['high'].append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#treat high urgency incidents different\n",
    "high_count_trns = model_scores[model_scores.bucket == 'high'][['trns_to_hosp']].sum()[0]\n",
    "high_count_not = len(model_scores[model_scores.bucket == 'high'][['trns_to_hosp']]) - high_count_trns\n",
    "high_indices = model_scores[model_scores.bucket == 'high'][['trns_to_hosp']].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_search(dfs, mods, curr_max, tot):\n",
    "    \"\"\"\n",
    "    input: dataframes of predicted scores for each bucket\n",
    "    AND which models they are coming from AND current best score AND total length of training set\n",
    "    output: updated best score\n",
    "    \"\"\"\n",
    "    #set k ranges\n",
    "    k1_range = (0.3, 0.8)\n",
    "    k2_range = (0.6, 0.9)\n",
    "    k3_range = (1, 1)\n",
    "    \n",
    "    #set step\n",
    "    step = 0.01\n",
    "    \n",
    "    #set timeout\n",
    "    quit_iters = 200\n",
    "\n",
    "    #true positive goal to hit\n",
    "    hit_goal = 0.4\n",
    "    \n",
    "    #accepted margin of error\n",
    "    margin = 0.05\n",
    "    \n",
    "    #create weights\n",
    "    w_score_high = [10,-5000,-1,0]\n",
    "    w_score_med = [3,-3,-1,1]\n",
    "    w_score_low = [1,-1,-1,1.7]\n",
    "\n",
    "    counter = 0.0\n",
    "    k_best = (0,0,0)\n",
    "    #tot = len(dfs[0]) + len(dfs[1]) + len(dfs[2])\n",
    "\n",
    "    low_scores = dfs[0].score\n",
    "    med_scores = dfs[1].score\n",
    "    \n",
    "\n",
    "    low_true = dfs[0].trns_to_hosp\n",
    "    med_true = dfs[1].trns_to_hosp\n",
    "    \n",
    "    #loop through all k values pairs updating best score at each iteration\n",
    "    for k1 in np.arange(k1_range[0], k1_range[1]+step, step):\n",
    "        \n",
    "        k1_level = np.percentile(low_scores, 100*(1 - k1))\n",
    "\n",
    "        low_preds = low_scores >= k1_level\n",
    "\n",
    "        tp_low = sum([i == 1 and j == 1 for i,j in zip(low_true, low_preds)])\n",
    "        fp_low = low_preds.sum() - tp_low\n",
    "        fn_low = sum([i == 1 and j == 0 for i,j in zip(low_true, low_preds)])\n",
    "        tn_low = sum([i == 0 and j == 0 for i,j in zip(low_true, low_preds)])\n",
    "        wsc_low = tp_low*w_score_low[0] + fp_low*w_score_low[2] + fn_low*w_score_low[1] + tn_low*w_score_low[3]\n",
    "\n",
    "        for k2 in np.arange(k2_range[0], k2_range[1]+step, step):\n",
    "            \n",
    "            k2_level = np.percentile(med_scores, 100*(1 - k2))\n",
    "\n",
    "            med_preds = med_scores >= k2_level\n",
    "\n",
    "            tp_med = sum([i == 1 and j == 1 for i,j in zip(med_true, med_preds)])\n",
    "            fp_med = med_preds.sum() - tp_med\n",
    "            fn_med = sum([i == 1 and j == 0 for i,j in zip(med_true, med_preds)])\n",
    "            tn_med = sum([i == 0 and j == 0 for i,j in zip(med_true, med_preds)])\n",
    "            wsc_med = tp_med*w_score_med[0] + fp_med*w_score_med[2] + fn_med*w_score_med[1] + tn_med*w_score_med[3]\n",
    "\n",
    "            \n",
    "\n",
    "            actual = (tn_low + tn_med)/float(tot)\n",
    "            \n",
    "            #only update if within margin of error and weighted score is better than before\n",
    "            if actual >= hit_goal - margin and actual  <= hit_goal + margin:\n",
    "                if wsc_low + wsc_med  > curr_max:\n",
    "                    k_best = (k1_level, k2_level)\n",
    "                    curr_max = wsc_low + wsc_med\n",
    "                    counter = 0\n",
    "                    print k_best, '|', curr_max, '|', actual, '|', mods\n",
    "                    print '------------------------------'\n",
    "                else:\n",
    "                    counter += 1\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= quit_iters:\n",
    "                    return curr_max\n",
    "                    \n",
    "    return curr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loop over all combinations of models\n",
    "curr_max = 0\n",
    "l = dict_buckets['low']\n",
    "random.shuffle(l)\n",
    "\n",
    "for low_mod in l:\n",
    "    low_slice = model_scores[model_scores.bucket == 'low']\n",
    "    low_slice = low_slice.dropna(axis=1, how='all')\n",
    "    model_slice_low = low_slice[['trns_to_hosp', low_mod]].dropna(axis=0)\n",
    "    model_slice_low['score'] = low_slice[low_mod]\n",
    "    model_slice_low.drop(low_mod, 1, inplace = True)\n",
    "    \n",
    "    for med_mod in np.random.choice(dict_buckets['med'],10):\n",
    "        med_slice = model_scores[model_scores.bucket == 'medium']\n",
    "        med_slice = med_slice.dropna(axis=1, how='all')\n",
    "        model_slice_med = med_slice[['trns_to_hosp', med_mod]].dropna(axis=0)\n",
    "        model_slice_med['score'] = med_slice[med_mod]\n",
    "        model_slice_med.drop(med_mod, 1, inplace = True)\n",
    "        \n",
    "    \n",
    "        print [low_mod, med_mod]\n",
    "        print '---'\n",
    "        \n",
    "        dfs = [model_slice_low, model_slice_med]\n",
    "\n",
    "        valid_indices = model_slice_low.index.tolist() + model_slice_med.index.tolist() + high_indices.tolist() \n",
    "\n",
    "        sub_df = model_scores.ix[valid_indices]\n",
    "\n",
    "        #run k search for each combo\n",
    "        curr_max = k_search(dfs, [low_mod, med_mod, 'NULL'], curr_max, len(sub_df))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(low_slice)/float(len(model_scores)), len(med_slice)/float(len(model_scores)), len(high_slice)/float(len(model_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Set Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#same as above, sanity check on train data\n",
    "curr_max = 0\n",
    "\n",
    "for low_mod in ['m_f69bddd49b08947e4d1e7f64090f411c']:\n",
    "    low_slice = model_scores[model_scores.bucket == 'low']\n",
    "    low_slice = low_slice.dropna(axis=1, how='all')\n",
    "    model_slice_low = low_slice[['trns_to_hosp', low_mod]].dropna(axis=0)\n",
    "    model_slice_low['score'] = low_slice[low_mod]\n",
    "    model_slice_low.drop(low_mod, 1, inplace = True)\n",
    "    \n",
    "    for med_mod in ['m_1786ecf969af47fbc1fbe8f26f959971']:\n",
    "        med_slice = model_scores[model_scores.bucket == 'medium']\n",
    "        med_slice = med_slice.dropna(axis=1, how='all')\n",
    "        model_slice_med = med_slice[['trns_to_hosp', med_mod]].dropna(axis=0)\n",
    "        model_slice_med['score'] = med_slice[med_mod]\n",
    "        model_slice_med.drop(med_mod, 1, inplace = True)\n",
    "        \n",
    "    \n",
    "        print [low_mod, med_mod]\n",
    "        print '---'\n",
    "        \n",
    "\n",
    "        dfs = [model_slice_low, model_slice_med]\n",
    "\n",
    "        valid_indices = model_slice_low.index.tolist() + model_slice_med.index.tolist()  + high_indices.tolist()\n",
    "\n",
    "        sub_df = model_scores.ix[valid_indices]\n",
    "\n",
    "       \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_scores = dfs[0].score\n",
    "med_scores = dfs[1].score\n",
    "\n",
    "\n",
    "low_true = dfs[0].trns_to_hosp\n",
    "med_true = dfs[1].trns_to_hosp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "k_set = [0.46250493856280006, 0.49599409868599997,0]\n",
    "\n",
    "scaler = 1\n",
    "\n",
    "tn_low = 0\n",
    "tn_med = 0\n",
    "\n",
    "\n",
    "tot = 1\n",
    "\n",
    "for item in [1]:\n",
    "    \n",
    "    k_vals = [i*item for i in k_set]\n",
    "    \n",
    "    if (tn_low + tn_med)/float(tot) <= 0.41 and (tn_low + tn_med)/float(tot) >= 0.39:\n",
    "        break\n",
    "        \n",
    "    tot = len(low_true) + len(med_true) \n",
    "\n",
    "    low_preds = low_scores >= k_vals[0]\n",
    "    med_preds = med_scores >= k_vals[1]\n",
    "   \n",
    "\n",
    "    tp_low = sum([i == 1 and j == 1 for i,j in zip(low_true, low_preds)])\n",
    "    fp_low = low_preds.sum() - tp_low\n",
    "    fn_low = sum([i == 1 and j == 0 for i,j in zip(low_true, low_preds)])\n",
    "    tn_low = sum([i == 0 and j == 0 for i,j in zip(low_true, low_preds)])\n",
    "\n",
    "    tp_med = sum([i == 1 and j == 1 for i,j in zip(med_true, med_preds)])\n",
    "    fp_med = med_preds.sum() - tp_med\n",
    "    fn_med = sum([i == 1 and j == 0 for i,j in zip(med_true, med_preds)])\n",
    "    tn_med = sum([i == 0 and j == 0 for i,j in zip(med_true, med_preds)])\n",
    "    \n",
    "    tot = len(low_preds) + len(med_preds) + high_count_trns + high_count_not\n",
    "\n",
    "print (tp_low + tp_med + high_count_trns)/float(tot)\n",
    "print (fn_low + fn_med)/float(tot)\n",
    "print (fp_low + fp_med + high_count_not)/float(tot)\n",
    "print (tn_low + tn_med)/float(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_df = pd.read_sql_query(\"SELECT incident, m_required FROM model.testing\", conn)\n",
    "\n",
    "#sub_df.set_index('Unnamed: 0',inplace=True)\n",
    "\n",
    "cur_df.set_index('incident',inplace=True)\n",
    "\n",
    "full = sub_df[['trns_to_hosp']].join(cur_df)\n",
    "\n",
    "tp = len(full[(full.trns_to_hosp == 1)&(full.m_required == True)])\n",
    "fn = len(full[(full.trns_to_hosp == 1)&(full.m_required == False)])\n",
    "fp = len(full[(full.trns_to_hosp == 0)&(full.m_required == True)])\n",
    "tn = len(full[(full.trns_to_hosp == 0)&(full.m_required == False)])\n",
    "all_len = tp+fn+fp+tn\n",
    "\n",
    "print float(tp)/all_len\n",
    "print float(fn)/all_len\n",
    "print float(fp)/all_len\n",
    "print float(tn)/all_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run same commands as above, now on test data using best models and best k values\n",
    "model_scores_test = pd.read_csv('/mnt/data/cincinnati/mkiang/cincinnati_ems/yes_time_all_model_scores.csv')\n",
    "model_scores_test.set_index('Unnamed: 0', inplace=True, drop=True)\n",
    "model_scores_test.index.rename('incident', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_count_trns = model_scores_test[model_scores_test.bucket == 'high'][['trns_to_hosp']].sum()[0]\n",
    "high_count_not = len(model_scores_test[model_scores_test.bucket == 'high'][['trns_to_hosp']]) - high_count_trns\n",
    "high_indices = model_scores_test[model_scores_test.bucket == 'high'][['trns_to_hosp']].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curr_max = 0\n",
    "\n",
    "for low_mod in ['m_94f214f6c2770dc60a4dff95de2b5a9b']:\n",
    "    low_slice = model_scores_test[model_scores_test.bucket == 'low']\n",
    "    low_slice = low_slice.dropna(axis=1, how='all')\n",
    "    model_slice_low = low_slice[['trns_to_hosp', low_mod]].dropna(axis=0)\n",
    "    model_slice_low['score'] = low_slice[low_mod]\n",
    "    model_slice_low.drop(low_mod, 1, inplace = True)\n",
    "    \n",
    "    for med_mod in ['m_ca6b4b8f069b5dd5c4e4bfd93de1e052']:\n",
    "        med_slice = model_scores_test[model_scores_test.bucket == 'medium']\n",
    "        med_slice = med_slice.dropna(axis=1, how='all')\n",
    "        model_slice_med = med_slice[['trns_to_hosp', med_mod]].dropna(axis=0)\n",
    "        model_slice_med['score'] = med_slice[med_mod]\n",
    "        model_slice_med.drop(med_mod, 1, inplace = True)\n",
    "        \n",
    "    \n",
    "        print [low_mod, med_mod]\n",
    "        print '---'\n",
    "        \n",
    "\n",
    "        dfs = [model_slice_low, model_slice_med]\n",
    "\n",
    "        valid_indices = model_slice_low.index.tolist() + model_slice_med.index.tolist()  + high_indices.tolist()\n",
    "\n",
    "        sub_df = model_scores_test.ix[valid_indices]\n",
    "\n",
    "       \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(low_slice)/float(len(model_scores_test)), len(med_slice)/float(len(model_scores_test)), len(high_slice)/float(len(model_scores_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_scores = dfs[0].score\n",
    "med_scores = dfs[1].score\n",
    "\n",
    "\n",
    "low_true = dfs[0].trns_to_hosp\n",
    "med_true = dfs[1].trns_to_hosp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scale back k's fixing proportions until we reach correct true negative percentage\n",
    "k_set = [0.45957257626522998, 0.48092488563031999,0]\n",
    "\n",
    "curr_max = 0\n",
    "\n",
    "for l in np.arange(0.5,0.3,-0.01):\n",
    "    for m in np.arange(0.5,0.3,-0.01):\n",
    "        k_set = [l,m,0]\n",
    "\n",
    "        scaler = 1\n",
    "\n",
    "        tn_low = 0\n",
    "        tn_med = 0\n",
    "\n",
    "\n",
    "        tot = 1\n",
    "\n",
    "        for item in np.arange(1.5,0,-0.05):\n",
    "\n",
    "            k_vals = [i*item for i in k_set]\n",
    "\n",
    "            if (tn_low + tn_med)/float(tot) <= 0.41 and (tn_low + tn_med)/float(tot) >= 0.39:\n",
    "                break\n",
    "\n",
    "            tot = len(low_true) + len(med_true) \n",
    "\n",
    "            low_preds = low_scores >= k_vals[0]\n",
    "            med_preds = med_scores >= k_vals[1]\n",
    "\n",
    "\n",
    "            tp_low = sum([i == 1 and j == 1 for i,j in zip(low_true, low_preds)])\n",
    "            fp_low = low_preds.sum() - tp_low\n",
    "            fn_low = sum([i == 1 and j == 0 for i,j in zip(low_true, low_preds)])\n",
    "            tn_low = sum([i == 0 and j == 0 for i,j in zip(low_true, low_preds)])\n",
    "\n",
    "            tp_med = sum([i == 1 and j == 1 for i,j in zip(med_true, med_preds)])\n",
    "            fp_med = med_preds.sum() - tp_med\n",
    "            fn_med = sum([i == 1 and j == 0 for i,j in zip(med_true, med_preds)])\n",
    "            tn_med = sum([i == 0 and j == 0 for i,j in zip(med_true, med_preds)])\n",
    "\n",
    "            tot = len(low_preds) + len(med_preds) + high_count_trns + high_count_not\n",
    "\n",
    "            print item, (tn_low + tn_med)/float(tot)\n",
    "        \n",
    "        if (tp_low + tp_med + high_count_trns)/float(tot) > curr_max and (tn_low + tn_med)/float(tot) <= 0.41 and (tn_low + tn_med)/float(tot) >= 0.39:\n",
    "            curr_max = (tp_low + tp_med + high_count_trns)/float(tot)\n",
    "            print '---'\n",
    "            print l,m,curr_max\n",
    "            print '---'\n",
    "\n",
    "print (tp_low + tp_med + high_count_trns)/float(tot)\n",
    "print (fn_low + fn_med)/float(tot)\n",
    "print (fp_low + fp_med + high_count_not)/float(tot)\n",
    "print (tn_low + tn_med)/float(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_df = pd.read_sql_query(\"SELECT incident, m_required FROM model.testing\", conn)\n",
    "\n",
    "#sub_df.set_index('Unnamed: 0',inplace=True)\n",
    "\n",
    "cur_df.set_index('incident',inplace=True)\n",
    "\n",
    "full = sub_df[['trns_to_hosp']].join(cur_df)\n",
    "\n",
    "tp = len(full[(full.trns_to_hosp == 1)&(full.m_required == True)])\n",
    "fn = len(full[(full.trns_to_hosp == 1)&(full.m_required == False)])\n",
    "fp = len(full[(full.trns_to_hosp == 0)&(full.m_required == True)])\n",
    "tn = len(full[(full.trns_to_hosp == 0)&(full.m_required == False)])\n",
    "all_len = len(full)\n",
    "\n",
    "print float(tp)/all_len\n",
    "print float(fn)/all_len\n",
    "print float(fp)/all_len\n",
    "print float(tn)/all_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feat Imps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#read json file of model we care about\n",
    "import json\n",
    "dict_info = json.loads(df[df.PICKLE == '/mnt/data/cincinnati/model_pickle/ca6b4b8f069b5dd5c4e4bfd93de1e052.p'].JSON.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_featimps = sorted(dict_info['MODEL FEATS & IMPS'], key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate list of importances for each feature class\n",
    "acs_sum = []\n",
    "weather_sum = []\n",
    "time_sum = []\n",
    "code_type_sum = []\n",
    "code_sev_sum = []\n",
    "call_source_sum = []\n",
    "operator_name_sum = []\n",
    "geography_sum = []\n",
    "building_sum = []\n",
    "past_sum = []\n",
    "\n",
    "for item in sorted_featimps:\n",
    "    if 'acs' in item[0]:\n",
    "        acs_sum.append(item[1])\n",
    "    elif 'weather' in item[0] or 'relative' in item[0]:\n",
    "        weather_sum.append(item[1])\n",
    "    elif 'time' in item[0]:\n",
    "        time_sum.append(item[1])\n",
    "    elif 'code_type' in item[0]:\n",
    "        code_type_sum.append(item[1])\n",
    "    elif 'code_level' in item[0]:\n",
    "        code_sev_sum.append(item[1])\n",
    "    elif 'call_source' in item[0]:\n",
    "        call_source_sum.append(item[1])\n",
    "    elif 'operator_name' in item[0]:\n",
    "        operator_name_sum.append(item[1])\n",
    "    elif 'building' in item[0]:\n",
    "        building_sum.append(item[1])\n",
    "    elif 'frac' in item[0] or 'total_' in item[0] or 'repeated_' in item[0]:\n",
    "        past_sum.append(item[1])\n",
    "    elif 'station' in item[0]:\n",
    "        geography_sum.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#take median of each class\n",
    "feat_dict = {'Demographics': np.median(acs_sum), 'Weather': np.median(weather_sum), 'Time': np.median(time_sum), 'Code Type': np.median(code_type_sum), 'Code Severity': np.median(code_sev_sum), 'Call Source': np.median(call_source_sum), 'Calltaker': np.median(operator_name_sum)\n",
    "             ,'Building Type': np.median(building_sum), 'Past History': np.median(past_sum), 'Geography': np.median(geography_sum)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feat_items = feat_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_items = sorted(feat_items, key = lambda x: -x[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#draw barchart of importances\n",
    "plt.figure(figsize=(24,6))\n",
    "mpl.rcdefaults()\n",
    "scaler = 1\n",
    "plt.bar(np.arange(0,scaler*len(feat_items), scaler), [i[1] for i in feat_items])\n",
    "plt.xticks(np.arange(0.4, scaler*(len(feat_items)+0.4),scaler),[i[0] for i in feat_items], fontsize = 40)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.ylabel('Importance', fontsize = 48)\n",
    "#plt.xlabel('Predictor', fontsize = 30)\n",
    "\n",
    "plt.yticks([])\n",
    "\n",
    "plt.xlim(-0.4, scaler*len(feat_items))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('feature_importanes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
